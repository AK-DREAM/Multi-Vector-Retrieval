{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5362df-aee2-4eaa-b21a-7e26e47d693f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/RAG/Decompose_retrieval/raptor')\n",
    "\n",
    "import cv2\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "import torch\n",
    "from image_utils import *\n",
    "import argparse\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from retrieval_utils import *\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from beir.retrieval.search.dense.exact_search import one, two, three, four\n",
    "from beir.retrieval import models\n",
    "from beir import LoggingHandler\n",
    "from datasets import load_dataset\n",
    "from datasets.download.download_config import DownloadConfig\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "import time\n",
    "# from beir.retrieval.models.clip_model import clip_model\n",
    "from clustering import *\n",
    "from text_utils import *\n",
    "import copy\n",
    "import os, shutil\n",
    "from bbox_utils import *\n",
    "from utils import *\n",
    "from sparse_index import *\n",
    "from baselines.llm_ranker import *\n",
    "from baselines.bm25 import *\n",
    "from derive_sub_query_dependencies import group_dependent_segments_seq_all\n",
    "import random\n",
    "from dessert_minheap_torch import *\n",
    "import pynvml\n",
    "from LLM4split.prompt_utils import *\n",
    "from raptor.raptor_embeddings import *\n",
    "\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "import base64\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "from PIL import Image as im\n",
    "from IPython.display import Image, display, Markdown\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "import torch\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e4d041-5109-4c7f-8ffe-2204f0c002d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_retrieval_datasets = [\"flickr\", \"AToMiC\", \"crepe\", \"crepe_full\", \"mscoco\", \"mscoco_40k\", \"multiqa\" ,\"multiqa_few\", \"art\"]\n",
    "text_retrieval_datasets = [\"trec-covid\", \"nq\", \"climate-fever\", \"hotpotqa\", \"msmarco\", \"webis-touche2020\", \"scifact\", \"fiqa\"]\n",
    "\n",
    "\n",
    "    \n",
    "def embed_queries(filename_ls, filename_cap_mappings, processor, model, device):\n",
    "    text_emb_ls = []\n",
    "    with torch.no_grad():\n",
    "        # for filename, caption in tqdm(filename_cap_mappings.items()):\n",
    "        for file_name in filename_ls:\n",
    "            caption = filename_cap_mappings[file_name]\n",
    "            inputs = processor(caption)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            text_features = model.get_text_features(**inputs)\n",
    "            # text_features = outputs.last_hidden_state[:, 0, :]\n",
    "            text_emb_ls.append(text_features.cpu())\n",
    "    return text_emb_ls\n",
    "\n",
    "def embed_queries_with_input_queries(model_name, query_ls, processor, model, device):\n",
    "    text_emb_ls = []\n",
    "    with torch.no_grad():\n",
    "        # for filename, caption in tqdm(filename_cap_mappings.items()):\n",
    "        for caption in query_ls:\n",
    "            # caption = filename_cap_mappings[file_name]\n",
    "            inputs = processor(caption)\n",
    "            if model_name == \"default\":\n",
    "                inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "                text_features = model.get_text_features(**inputs)\n",
    "            elif model_name == \"blip\":\n",
    "                text_features = model.extract_features({\"text_input\":inputs}, mode=\"text\").text_embeds_proj[:,0,:].view(1,-1)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model name\")\n",
    "            # text_features = outputs.last_hidden_state[:, 0, :]\n",
    "            text_emb_ls.append(text_features.cpu())\n",
    "    return text_emb_ls\n",
    "\n",
    "def embed_queries_ls(model_name, full_sub_queries_ls, processor, model, device):\n",
    "    text_emb_ls = []\n",
    "    with torch.no_grad():\n",
    "        # for filename, caption in tqdm(filename_cap_mappings.items()):\n",
    "        # for file_name in filename_ls:\n",
    "        for sub_queries_ls in tqdm(full_sub_queries_ls):\n",
    "            sub_text_emb_ls = []\n",
    "            for sub_queries in sub_queries_ls:\n",
    "                sub_text_feature_ls = []\n",
    "                for subquery in sub_queries:\n",
    "                    # caption = filename_cap_mappings[file_name]\n",
    "                    inputs = processor(subquery)\n",
    "                    if model_name == \"default\":\n",
    "                        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "                        text_features = model.get_text_features(**inputs)\n",
    "                    elif model_name == \"blip\":\n",
    "                        text_features = model.extract_features({\"text_input\":inputs}, mode=\"text\").text_embeds_proj[:,0,:].view(1,-1)\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid model name\")\n",
    "                    sub_text_feature_ls.append(text_features.cpu())\n",
    "                # text_features = outputs.last_hidden_state[:, 0, :]\n",
    "                sub_text_emb_ls.append(sub_text_feature_ls)\n",
    "            text_emb_ls.append(sub_text_emb_ls)\n",
    "    return text_emb_ls\n",
    "\n",
    "        \n",
    "def retrieve_by_full_query(img_emb, text_emb_ls):\n",
    "    text_emb_tensor = torch.cat(text_emb_ls).cpu()\n",
    "    scores = (img_emb @ text_emb_tensor.T).squeeze()/ (img_emb.norm(dim=-1) * text_emb_tensor.norm(dim=-1))\n",
    "    true_rank = torch.tensor([i for i in range(len(text_emb_ls))])\n",
    "    top_k_acc = top_k_accuracy_score(true_rank, scores, k=1)\n",
    "    \n",
    "    print(f\"Top-k accuracy: {top_k_acc:.2f}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # for idx in range(len(text_emb_ls)):\n",
    "    #     text_emb = text_emb_ls[idx]\n",
    "    #     scores = (img_emb @ text_emb.T).squeeze()/ (img_emb.norm(dim=-1) * text_emb.norm(dim=-1))\n",
    "        \n",
    "        # print(scores)\n",
    "        # print(scores.shape)\n",
    "        # print(scores.argmax())\n",
    "        # print()\n",
    "    \n",
    "def set_rand_seed(seed_value):\n",
    "    # Set seed for Python's built-in random module\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed_value)\n",
    "    \n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    # Set seed for CUDA (if using a GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # If using multi-GPU.\n",
    "    \n",
    "    # Ensure deterministic operations for cuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='CUB concept learning')\n",
    "    parser.add_argument('--data_path', type=str, default=\"/data6/wuyinjun/\", help='config file')\n",
    "    parser.add_argument('--dataset_name', type=str, default=\"crepe\", help='config file')\n",
    "    parser.add_argument('--model_name', type=str, default=\"default\", help='config file')\n",
    "    parser.add_argument('--query_count', type=int, default=-1, help='config file')\n",
    "    parser.add_argument('--random_seed', type=int, default=0, help='config file')\n",
    "    parser.add_argument('--query_concept', action=\"store_true\", help='config file')\n",
    "    parser.add_argument('--img_concept', action=\"store_true\", help='config file')\n",
    "    parser.add_argument('--total_count', type=int, default=-1, help='config file')\n",
    "    parser.add_argument(\"--parallel\", action=\"store_true\", help=\"config file\")\n",
    "    parser.add_argument(\"--save_mask_bbox\", action=\"store_true\", help=\"config file\")\n",
    "    parser.add_argument(\"--search_by_cluster\", action=\"store_true\", help=\"config file\")\n",
    "    parser.add_argument('--algebra_method', type=str, default=one, help='config file')\n",
    "    # closeness_threshold\n",
    "    parser.add_argument('--closeness_threshold', type=float, default=0.1, help='config file')\n",
    "    parser.add_argument('--subset_img_id', type=int, default=None, help='config file')\n",
    "    parser.add_argument('--prob_agg', type=str, default=\"prod\", choices=[\"prod\", \"sum\"], help='config file')\n",
    "    parser.add_argument('--segmentation_method', type=str, default=\"default\", choices=[\"default\", \"scene_graph\"], help='config file')\n",
    "    parser.add_argument('--dependency_topk', type=int, default=20, help='config file')\n",
    "    parser.add_argument('--clustering_topk', type=int, default=500, help='config file')\n",
    "    parser.add_argument(\"--add_sparse_index\", action=\"store_true\", help=\"config file\")\n",
    "    \n",
    "    parser.add_argument('--retrieval_method', type=str, default=\"ours\", help='config file')\n",
    "    parser.add_argument('--index_method', type=str, default=\"default\", choices=[\"default\", \"dessert\", \"dessert0\"], help='config file')\n",
    "    parser.add_argument('--hashes_per_table', type=int, default=5, help='config file')\n",
    "    # num_tables\n",
    "    parser.add_argument('--num_tables', type=int, default=100, help='config file')\n",
    "    parser.add_argument('--clustering_doc_count_factor', type=int, default=1, help='config file')\n",
    "    parser.add_argument('--clustering_number', type=float, default=0.1, help='config file')\n",
    "    # \n",
    "    parser.add_argument('--nprobe_query', type=int, default=2, help='config file')\n",
    "    parser.add_argument('--subset_patch_count', type=int, default=-1, help='config file')\n",
    "    parser.add_argument('--cached_file_suffix', type=str, default=\"\", help='config file')\n",
    "    parser.add_argument(\"--is_test\", action=\"store_true\", help=\"config file\")\n",
    "    parser.add_argument(\"--store_res\", action=\"store_true\", help=\"config file\")\n",
    "    parser.add_argument(\"--use_phi\", action=\"store_true\", help=\"config file\")\n",
    "    parser.add_argument('--use_raptor', action=\"store_true\", help='config file')\n",
    "  \n",
    "    args = parser.parse_args(args=['--dataset_name', 'multiqa',\n",
    "                                   '--data_path', '/root/autodl-fs/datasets/',\n",
    "                                   '--query_concept', '--img_concept', '--algebra_method', 'two'])\n",
    "    return args\n",
    "\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def obtain_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    # print(f\"Memory usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "    memory_usage = process.memory_info().rss / 1024 ** 3\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def obtain_gpu_memory_usage():\n",
    "    # pynvml.nvmlInit()\n",
    "    current_pid = os.getpid()\n",
    "    \n",
    "    used_gpu_memory = -1\n",
    "    pynvml.nvmlInit()\n",
    "    for dev_id in range(pynvml.nvmlDeviceGetCount()):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(dev_id)\n",
    "        for proc in pynvml.nvmlDeviceGetComputeRunningProcesses(handle):\n",
    "            if proc.pid == current_pid:\n",
    "                used_gpu_memory = proc.usedGpuMemory\n",
    "            \n",
    "    #         print(\n",
    "    #             \"pid %d using %d bytes of memory on device %d.\"\n",
    "    #             % (proc.pid, proc.usedGpuMemory, dev_id)\n",
    "    #         )\n",
    "    \n",
    "    # handle = pynvml.nvmlDeviceGetHandleByIndex(3)\n",
    "\n",
    "    # # Get the list of processes using the GPU\n",
    "    # processes = pynvml.nvmlDeviceGetGraphicsRunningProcesses_v2(handle)\n",
    "    \n",
    "    # for process in processes:\n",
    "    #     if process.pid == current_pid:\n",
    "    #         used_memory = process.usedGpuMemory / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "    # process = psutil.Process(os.getpid())\n",
    "    # # print(f\"Memory usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "    # memory_usage = process.memory_info().rss / 1024 ** 2\n",
    "    return used_gpu_memory / 1024 ** 3\n",
    "\n",
    "\n",
    "def construct_qrels(dataset_name, queries, cached_img_idx, img_idx_ls, query_count):\n",
    "    qrels = {}\n",
    "    # if query_count < 0:\n",
    "    #     query_count = \n",
    "    \n",
    "    for idx in range(len(queries)):\n",
    "        curr_img_idx = img_idx_ls[idx]\n",
    "        cached_idx = cached_img_idx.index(curr_img_idx)\n",
    "        qrels[str(idx+1)] = {str(cached_idx+1): 2}\n",
    "    q_idx_ls = list(range(len(queries)))\n",
    "    if query_count > 0:\n",
    "        if dataset_name == \"crepe\":\n",
    "            subset_q_idx_ls = [q_idx_ls[idx] for idx in range(query_count)] \n",
    "        else:\n",
    "            subset_q_idx_ls = random.sample(q_idx_ls, query_count)\n",
    "        \n",
    "        subset_q_idx_ls = sorted(subset_q_idx_ls)\n",
    "        \n",
    "        subset_qrels = {str(key_id + 1): qrels[str(subset_q_idx_ls[key_id] + 1)] for key_id in range(len(subset_q_idx_ls))}\n",
    "    \n",
    "        qrels = subset_qrels\n",
    "        \n",
    "        queries = [queries[idx] for idx in subset_q_idx_ls]\n",
    "    else:\n",
    "        subset_q_idx_ls = q_idx_ls #list(qrels.keys())\n",
    "        \n",
    "    return qrels, queries, subset_q_idx_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c1f725-4cab-40e3-9a56-7d406572bb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57025it [00:00, 304633.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached patches\n",
      "5124ef94ce5cf36253a7e93571244cc05d029e016b06af178e05f11976c9ed52\n",
      "img_for_patch_tensor: torch.Size([143319])\n",
      "patch_activations: torch.Size([143319, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57025/57025 [00:13<00:00, 4354.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached patches\n",
      "5124ef94ce5cf36253a7e93571244cc05d029e016b06af178e05f11976c9ed52\n",
      "img_for_patch_tensor: torch.Size([262935])\n",
      "patch_activations: torch.Size([262935, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57025/57025 [00:13<00:00, 4116.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached patches\n",
      "5124ef94ce5cf36253a7e93571244cc05d029e016b06af178e05f11976c9ed52\n",
      "img_for_patch_tensor: torch.Size([576226])\n",
      "patch_activations: torch.Size([576226, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57025/57025 [00:22<00:00, 2522.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached patches\n",
      "5124ef94ce5cf36253a7e93571244cc05d029e016b06af178e05f11976c9ed52\n",
      "img_for_patch_tensor: torch.Size([2627371])\n",
      "patch_activations: torch.Size([2627371, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57025/57025 [01:21<00:00, 698.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample hash:: 5124ef94ce5cf36253a7e93571244cc05d029e016b06af178e05f11976c9ed52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57025/57025 [00:09<00:00, 6180.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load bbox neighbor information from file:  output/bboxes_overlap_5124ef94ce5cf36253a7e93571244cc05d029e016b06af178e05f11976c9ed52_4_8_16_64.pkl\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                level=logging.INFO,\n",
    "                handlers=[LoggingHandler()])\n",
    "used_memory0 = psutil.virtual_memory().used\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "\n",
    "args.is_img_retrieval = args.dataset_name in image_retrieval_datasets\n",
    "set_rand_seed(args.random_seed)\n",
    "\n",
    "# args.query_concept = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = ViTImageProcessor.from_pretrained('google/vit-large-patch16-224')\n",
    "# model = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224').to(device)\n",
    "if args.is_img_retrieval:\n",
    "    if args.model_name == \"default\":\n",
    "       \n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "        # processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        raw_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        # processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\", padding=False, do_resize=False, do_center_crop=False)[\"pixel_values\"]\n",
    "        # processor = lambda images: safe_raw_processor(images)\n",
    "        \n",
    "        processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "        text_processor =  lambda text: raw_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        img_processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        model = model.eval()\n",
    "        \n",
    "    elif args.model_name == \"blip\":\n",
    "        print(\"start loading blip model\")\n",
    "        if os.path.exists(\"output/blip.pkl\"):\n",
    "            model,vis_processors_eval, txt_processors_eval = utils.load(\"output/blip.pkl\")\n",
    "        else:\n",
    "            from lavis.models import load_model_and_preprocess\n",
    "            model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_feature_extractor\", model_type=\"base\", is_eval=True, device=device)\n",
    "            utils.save((model,vis_processors[\"eval\"], txt_processors[\"eval\"]), \"output/blip.pkl\")\n",
    "            txt_processors_eval = txt_processors[\"eval\"]\n",
    "            vis_processors_eval = vis_processors[\"eval\"]\n",
    "        text_processor = lambda text: txt_processors_eval(text)\n",
    "        processor =  lambda images: torch.stack([vis_processors_eval(image) for image in images])\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    \n",
    "    if args.add_sparse_index:\n",
    "        # text_model = models.SentenceBERT(\"msmarco-distilbert-base-tas-b\", prefix = sparse_prefix, suffix=sparse_suffix)\n",
    "        # if args.model_name == \"default\":\n",
    "        print(\"start loading distill-bert model\")\n",
    "        if not os.path.exists(\"output/msmarco-distilbert-base-tas-b.pkl\"):\n",
    "            text_model = models.SentenceBERT(\"msmarco-distilbert-base-tas-b\", prefix = sparse_prefix, suffix=sparse_suffix) # offline model\n",
    "            utils.save(text_model, \"output/msmarco-distilbert-base-tas-b.pkl\")\n",
    "        else:\n",
    "            text_model = utils.load(\"output/msmarco-distilbert-base-tas-b.pkl\")\n",
    "    \n",
    "else:\n",
    "    # if args.dataset_name not in image_retrieval_datasets:\n",
    "    # if not args.is_img_retrieval:\n",
    "    # text_model = models.clip_model(text_processor, model, device)\n",
    "    if args.model_name == \"default\":\n",
    "        print(\"start loading distill-bert model\")\n",
    "        if True: #not os.path.exists(\"output/msmarco-distilbert-base-tas-b.pkl\"):\n",
    "            text_model = models.SentenceBERT(\"msmarco-distilbert-base-tas-b\", prefix = sparse_prefix, suffix=sparse_suffix) # offline model\n",
    "            utils.save(text_model, \"output/msmarco-distilbert-base-tas-b.pkl\")\n",
    "        # else:\n",
    "        #     text_model = utils.load(\"output/msmarco-distilbert-base-tas-b.pkl\")\n",
    "    # elif args.model_name == \"phi\":\n",
    "    #     text_model = models.ms_phi(prefix=sparse_prefix, suffix=sparse_suffix)\n",
    "    elif args.model_name == \"llm\":\n",
    "        # text_model = models.LlmtoVec((\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
    "        #     (\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\",\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\"))\n",
    "        text_model = models.LlmtoVec((\"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\", \"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\"),\n",
    "            (\"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised\", \"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised\"))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    # text_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #             \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    #             device_map=\"cuda\", \n",
    "    #             torch_dtype=\"auto\", \n",
    "    #             trust_remote_code=True, \n",
    "    #         )\n",
    "    text_retrieval_model = DRES(batch_size=16)\n",
    "    # retriever = EvaluateRetrieval(text_model, score_function=\"cos_sim\") # or \"cos_sim\" for cosine similarity\n",
    "\n",
    "    # text_processor = AutoProcessor.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n",
    "    # model = models.SentenceBERT(\"msmarco-distilbert-base-tas-b\")\n",
    "    # model = model.eval()\n",
    "\n",
    "\n",
    "full_data_path = os.path.join(args.data_path, args.dataset_name)\n",
    "if args.dataset_name.startswith(\"crepe\"):\n",
    "    full_data_path = os.path.join(args.data_path, \"crepe\")\n",
    "\n",
    "# change 1\n",
    "# query_path = os.path.dirname(os.path.realpath(__file__)) \n",
    "query = 'What color is the Santa Anita Park logo?'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(full_data_path):\n",
    "    os.makedirs(full_data_path)\n",
    "\n",
    "pipe, generation_args= None, None\n",
    "if args.use_phi:\n",
    "    pipe, generation_args = init_phi_utils()\n",
    "    args.cached_file_suffix=\"_phi\"\n",
    "\n",
    "# origin_corpus = None\n",
    "bboxes_ls = None\n",
    "grouped_sub_q_ids_ls = None\n",
    "bboxes_overlap_ls = None\n",
    "img_file_name_ls = None\n",
    "\n",
    "# , bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=bboxes_overlap_ls, grouped_sub_q_ids_ls=grouped_sub_q_ids_ls\n",
    "\n",
    "\n",
    "if args.dataset_name == \"art\":\n",
    "    queries, img_file_name_ls, sub_queries_ls, img_idx_ls, grouped_sub_q_ids_ls = load_art_datasets(full_data_path, query, subset_img_id=args.subset_img_id, is_test=args.is_test)\n",
    "\n",
    "elif args.dataset_name == \"multiqa\":\n",
    "    queries, img_file_name_ls, sub_queries_ls, img_idx_ls, grouped_sub_q_ids_ls = load_art_datasets(full_data_path, query, subset_img_id=args.subset_img_id, is_test=args.is_test)\n",
    "\n",
    "elif args.dataset_name == \"multiqa_few\":\n",
    "    queries, img_file_name_ls, sub_queries_ls, img_idx_ls, grouped_sub_q_ids_ls = load_multiqafew_datasets(full_data_path, query_path, subset_img_id=args.subset_img_id, is_test=args.is_test)\n",
    "\n",
    "\n",
    "\n",
    "if args.is_img_retrieval:\n",
    "    # if not args.query_concept:    \n",
    "    #     patch_count_ls = [4, 8, 16, 32, 64, 128]\n",
    "    # else:\n",
    "        # patch_count_ls = [4, 8, 16, 32, 64, 128]\n",
    "        if args.dataset_name.startswith(\"crepe\"):\n",
    "            patch_count_ls = [4,8,16, 64,128]\n",
    "            # patch_count_ls = [32]\n",
    "        elif args.dataset_name.startswith(\"mscoco\"):\n",
    "            # patch_count_ls = [4, 8, 16, 64, 128]\n",
    "            patch_count_ls = [4, 8, 16, 64]\n",
    "            # patch_count_ls = [4, 16, 64]\n",
    "        else:\n",
    "            patch_count_ls = [4, 8, 16, 64]   #use this\n",
    "            \n",
    "        if args.segmentation_method == \"scene_graph\":\n",
    "            patch_count_ls = [1, 4, 8, 16, 32]\n",
    "else:\n",
    "    # patch_count_ls = [8, 24, 32]\n",
    "    patch_count_ls = [1, 16, 8, 4, 32]\n",
    "    # if not args.dataset_name == \"fiqa\":\n",
    "    #     patch_count_ls = [1, 4, 8, 16, 32]\n",
    "    # else:\n",
    "    #     patch_count_ls = [4, 32, 128, 256]\n",
    "    # patch_count_ls = [1]\n",
    "    # patch_count_ls = [32]\n",
    "\n",
    "if args.subset_patch_count > 0 and args.subset_patch_count < len(patch_count_ls):\n",
    "    patch_count_ls = patch_count_ls[:args.subset_patch_count]\n",
    "\n",
    "if args.is_img_retrieval:\n",
    "    # print('to here~~~~~~~~1')\n",
    "    samples_hash = obtain_sample_hash(img_idx_ls, img_file_name_ls)\n",
    "    # cached_img_idx_ls, image_embs, patch_activations, masks, bboxes, img_for_patch\n",
    "    # if args.save_mask_bbox:\n",
    "\n",
    "    cached_img_ls, img_emb, patch_emb_ls, _, bboxes_ls, img_per_patch_ls = convert_samples_to_concepts_img(args, samples_hash, model, img_file_name_ls, img_idx_ls, processor, device, patch_count_ls=patch_count_ls,save_mask_bbox=args.save_mask_bbox)  # debug\n",
    "    # else:\n",
    "    #     cached_img_ls, img_emb, patch_emb_ls, img_per_patch_ls = convert_samples_to_concepts_img(args, samples_hash, model, img_file_name_ls, img_idx_ls, processor, device, patch_count_ls=patch_count_ls,save_mask_bbox=args.save_mask_bbox)\n",
    "        \n",
    "    if args.add_sparse_index:\n",
    "        img_sparse_emb = construct_dense_or_sparse_encodings(args, corpus, text_model, samples_hash, is_sparse=True)\n",
    "        store_sparse_index(samples_hash, img_sparse_emb, encoding_query = False)\n",
    "\n",
    "elif args.dataset_name in text_retrieval_datasets:\n",
    "    \n",
    "    if (args.use_raptor):\n",
    "        raptor_model = RaptorEmbeddingGenerator()\n",
    "    else:\n",
    "        raptor_model = None\n",
    "    samples_hash,(img_emb, img_sparse_index), patch_emb_ls, bboxes_ls = convert_samples_to_concepts_txt(args, text_model, corpus, device, raptor_model=raptor_model, patch_count_ls=patch_count_ls)\n",
    "    \n",
    "    # img_emb = text_model.encode_corpus(corpus)\n",
    "    # if args.img_concept:\n",
    "    #     _,img_per_patch_ls, patch_emb_ls = generate_patch_ids_ls(patch_emb_ls)\n",
    "else:\n",
    "    print(\"Invalid dataset name, exit!\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"sample hash::\", samples_hash)\n",
    "patch_emb_by_img_ls = patch_emb_ls\n",
    "if args.img_concept:\n",
    "    # if args.is_img_retrieval:\n",
    "    patch_emb_by_img_ls, bboxes_ls = reformat_patch_embeddings(patch_emb_ls, None, img_emb, bbox_ls=bboxes_ls)    # debug\n",
    "    # else:\n",
    "    #     patch_emb_by_img_ls, bboxes_ls = reformat_patch_embeddings(patch_emb_ls, img_per_patch_ls, img_emb, bbox_ls=bboxes_ls)\n",
    "    \n",
    "    # if args.is_img_retrieval:\n",
    "\n",
    "    \n",
    "    \n",
    "sample_patch_ids_to_cluster_id_mappings = None\n",
    "if args.search_by_cluster:\n",
    "    if args.index_method == \"dessert\":\n",
    "        import dessert_py_dependency\n",
    "    if args.img_concept:\n",
    "        \n",
    "        # patch_clustering_info_cached_file = get_clustering_res_file_name(args, patch_count_ls)\n",
    "        if type(patch_emb_by_img_ls) is list:\n",
    "            patch_emb_by_img_ls = [torch.nn.functional.normalize(all_sub_corpus_embedding, p=2, dim=1) for all_sub_corpus_embedding in patch_emb_by_img_ls]\n",
    "        else:\n",
    "            patch_emb_by_img_ls = torch.nn.functional.normalize(patch_emb_by_img_ls, p=2, dim=1)\n",
    "\n",
    "        \n",
    "        patch_clustering_info_cached_file = get_dessert_clustering_res_file_name(args, samples_hash, patch_count_ls, clustering_number=args.clustering_number, index_method=args.index_method, typical_doclen=args.clustering_doc_count_factor, num_tables=args.num_tables, hashes_per_table=args.hashes_per_table)\n",
    "        \n",
    "        if not os.path.exists(patch_clustering_info_cached_file):\n",
    "        \n",
    "            centroid_file_name = get_clustering_res_file_name(args, samples_hash, patch_count_ls)\n",
    "            if os.path.exists(centroid_file_name):\n",
    "                centroids = torch.load(centroid_file_name)\n",
    "            else:\n",
    "                centroids =sampling_and_clustering(patch_emb_by_img_ls, dataset_name=args.dataset_name, clustering_number=args.clustering_number, typical_doclen=args.clustering_doc_count_factor)\n",
    "                torch.save(centroids, centroid_file_name)\n",
    "            # centroids = torch.zeros([1, patch_emb_by_img_ls[-1].shape[-1]])\n",
    "            # hashes_per_table: int, num_tables\n",
    "            max_patch_count = max([len(patch_emb_by_img_ls[idx]) for idx in range(len(patch_emb_by_img_ls))])\n",
    "            if not args.index_method == \"dessert\":\n",
    "                retrieval_method = DocRetrieval(max_patch_count, args.hashes_per_table, args.num_tables, patch_emb_by_img_ls[-1].shape[-1], centroids, device=device)\n",
    "            else:\n",
    "                retrieval_method = dessert_py_dependency.DocRetrieval(hashes_per_table = args.hashes_per_table, num_tables = args.num_tables, dense_input_dimension = patch_emb_by_img_ls[-1].shape[-1], nprobe_query=args.nprobe_query, centroids = centroids.detach().cpu().numpy().astype(np.float32));\n",
    "\n",
    "            for idx in tqdm(range(len(patch_emb_by_img_ls)), desc=\"add doc\"):\n",
    "                if not args.index_method == \"dessert\":\n",
    "                    retrieval_method.add_doc(patch_emb_by_img_ls[idx], idx, index_method=args.index_method)\n",
    "                else:\n",
    "                    retrieval_method.add_doc(patch_emb_by_img_ls[idx].detach().cpu().numpy().astype(np.float32), str(idx))\n",
    "            \n",
    "            # utils.save(retrieval_method, \"output/retrieval_method.pkl\")\n",
    "            if not args.index_method == \"dessert\":\n",
    "                utils.save(retrieval_method, patch_clustering_info_cached_file)\n",
    "            else:\n",
    "                retrieval_method.serialize_to_file(patch_clustering_info_cached_file)\n",
    "        else:\n",
    "            if not args.index_method == \"dessert\":\n",
    "                retrieval_method = utils.load(patch_clustering_info_cached_file)\n",
    "            else:\n",
    "                retrieval_method = dessert_py_dependency.DocRetrieval.deserialize_from_file(patch_clustering_info_cached_file)\n",
    "            \n",
    "    else:\n",
    "        patch_clustering_info_cached_file = get_dessert_clustering_res_file_name(samples_hash, [-1], clustering_number=args.clustering_number, index_method=args.index_method, typical_doclen=args.clustering_doc_count_factor)\n",
    "        patch_emb_by_img_ls = [img_emb[idx].view(1,-1) for idx in range(len(img_emb))]\n",
    "        if not os.path.exists(patch_clustering_info_cached_file):\n",
    "        \n",
    "            centroid_file_name = get_clustering_res_file_name(args, samples_hash, [-1])\n",
    "            if os.path.exists(centroid_file_name):\n",
    "                centroids = torch.load(centroid_file_name)\n",
    "            else:\n",
    "                centroids =sampling_and_clustering(patch_emb_by_img_ls, dataset_name=args.dataset_name, clustering_number=args.clustering_number, typical_doclen=args.clustering_doc_count_factor)\n",
    "                torch.save(centroids, centroid_file_name)\n",
    "            # centroids = torch.zeros([1, patch_emb_by_img_ls[-1].shape[-1]])\n",
    "            # hashes_per_table: int, num_tables\n",
    "            max_patch_count = max([len(patch_emb_by_img_ls[idx]) for idx in range(len(patch_emb_by_img_ls))])\n",
    "            retrieval_method = DocRetrieval(max_patch_count, args.hashes_per_table, args.num_tables, patch_emb_by_img_ls[-1].shape[-1], centroids, device=device)\n",
    "\n",
    "            for idx in tqdm(range(len(patch_emb_by_img_ls)), desc=\"add doc\"):\n",
    "                retrieval_method.add_doc(patch_emb_by_img_ls[idx], idx, index_method=args.index_method)\n",
    "            \n",
    "            # utils.save(retrieval_method, \"output/retrieval_method.pkl\")\n",
    "            utils.save(retrieval_method, patch_clustering_info_cached_file)\n",
    "        else:\n",
    "            retrieval_method = utils.load(patch_clustering_info_cached_file)\n",
    "        # cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_unique_sample_ids_ls, cluster_sample_ids_ls, cluster_sub_X_patch_ids_ls, cluster_sub_X_granularity_ids_ls\n",
    "        # cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_unique_sample_ids_ls,cluster_sample_ids_ls, cluster_sub_X_patch_ids_ls, cluster_sub_X_granularity_ids_ls, cluster_sub_X_cat_patch_ids_ls, sample_patch_ids_to_cluster_id_mappings = clustering_img_patch_embeddings(patch_emb_by_img_ls, args.dataset_name + \"_\" + str(args.total_count), patch_emb_ls, closeness_threshold=args.closeness_threshold)\n",
    "    if not args.index_method == \"dessert\":\n",
    "        retrieval_method._centroids = torch.nn.functional.normalize(retrieval_method._centroids, p=2, dim=0)\n",
    "        print(\"centroid shape::\", retrieval_method._centroids.shape)\n",
    "        \n",
    "        \n",
    "        # if False: #os.path.exists(patch_clustering_info_cached_file):\n",
    "        #     cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_unique_sample_ids_ls,cluster_sample_ids_ls, cluster_sub_X_patch_ids_ls, cluster_sub_X_granularity_ids_ls, cluster_sub_X_cat_patch_ids_ls = utils.load(patch_clustering_info_cached_file)\n",
    "        # else: \n",
    "        #     utils.save((cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_unique_sample_ids_ls,cluster_sample_ids_ls, cluster_sub_X_patch_ids_ls, cluster_sub_X_granularity_ids_ls, cluster_sub_X_cat_patch_ids_ls, sample_patch_ids_to_cluster_id_mappings), patch_clustering_info_cached_file)\n",
    "    # else:\n",
    "    #     cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_sample_ids_ls = clustering_img_embeddings(img_emb)\n",
    "\n",
    "if args.img_concept:\n",
    "    bboxes_overlap_ls, clustering_nbs_mappings = init_bbox_nbs(args, patch_count_ls, samples_hash, bboxes_ls, patch_emb_by_img_ls, sample_patch_ids_to_cluster_id_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14eb570-6b9e-450e-9874-05f11e138d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "2024-12-03 16:43:16,159 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21612d1d3da8409b94216efdd71cf932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:43:23,690 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/root/autodl-tmp/LLaVA/llava-v1.5-7b\"\n",
    "def print_results(results):\n",
    "    for idx, uri in enumerate(results['uris'][0]):\n",
    "        print(f\"ID: {results['ids'][0][idx]}\")\n",
    "        print(f\"Distance: {results['distances'][0][idx]}\")\n",
    "        print(f\"Path: {uri}\")\n",
    "        display(Image(filename=uri, width=300))\n",
    "        print(\"\\n\")\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = im.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        # print(image_file)\n",
    "        image = im.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "    \n",
    "def infer_once(query, image_file):\n",
    "    model_name = get_model_name_from_path(model_path)\n",
    "    conv_m = None\n",
    "    \n",
    "    qs = query\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if vlm_model.config.mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if vlm_model.config.mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    if conv_m is not None and conv_mode != conv_m:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, conv_m, conv_m\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        conv_m = conv_mode\n",
    "\n",
    "    conv = conv_templates[conv_m].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    image_files = image_file\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        vlm_model.config\n",
    "    ).to(vlm_model.device, dtype=torch.float16)\n",
    "    \n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, vlm_tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda()\n",
    "    )\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = vlm_model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=True if 0 > 0 else False,\n",
    "            temperature=0,\n",
    "            top_p=None,\n",
    "            num_beams=1,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    outputs = vlm_tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    filename = os.path.basename(image_file[0])\n",
    "    # # 去除文件扩展名，仅保留文件名\n",
    "    # file_id = os.path.splitext(filename)[0]\n",
    "    return outputs, filename\n",
    "\n",
    "\n",
    "disable_torch_init()\n",
    "vlm_tokenizer, vlm_model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=get_model_name_from_path(model_path) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e084e77c-8239-47ff-a277-031e4be210d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_once(query, patch_emb_by_img_ls):\n",
    "    queries = []  \n",
    "    sub_queries_ls = [] \n",
    "    grouped_sub_q_ids_ls = None\n",
    "\n",
    "    sparse_sim_scores = None\n",
    "\n",
    "    \n",
    "    queries.append(query)\n",
    "    tmp_ls = [[query]]\n",
    "    sub_queries_ls.append(tmp_ls)\n",
    "    \n",
    "    \n",
    "    if args.is_img_retrieval:\n",
    "                qrels, queries, subset_q_idx = construct_qrels(args.dataset_name, queries, cached_img_ls, img_idx_ls, query_count=args.query_count)\n",
    "                if args.query_count > 0:\n",
    "                    sub_queries_ls = [sub_queries_ls[idx] for idx in subset_q_idx]\n",
    "                    grouped_sub_q_ids_ls = [grouped_sub_q_ids_ls[idx] for idx in subset_q_idx]\n",
    "        \n",
    "        \n",
    "    if args.is_img_retrieval:\n",
    "        if args.query_concept:\n",
    "            full_sub_queries_ls = [sub_queries_ls[idx] + [[queries[idx]]] for idx in range(len(sub_queries_ls))] \n",
    "            text_emb_ls = embed_queries_ls(args.model_name, full_sub_queries_ls, text_processor, model, device)\n",
    "        else:\n",
    "            text_emb_ls = embed_queries_with_input_queries(args.model_name, queries, text_processor, model, device)\n",
    "            full_sub_queries_ls = []  # change-1\n",
    "    else:\n",
    "        if not args.query_concept:\n",
    "            # text_emb_ls = text_model.encode_queries(queries, convert_to_tensor=True)\n",
    "            text_emb_ls, _ = construct_dense_or_sparse_encodings_queries(queries, text_model, args.add_sparse_index)\n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            full_sub_queries_ls = sub_queries_ls\n",
    "            # full_sub_queries_ls = [sub_queries_ls[idx] + [[reformated_queries[idx]]] for idx in range(len(sub_queries_ls))]\n",
    "            text_emb_ls = encode_sub_queries_ls(full_sub_queries_ls, text_model)\n",
    "            text_emb_dense = text_model.encode_queries(queries, convert_to_tensor=True)\n",
    "            text_emb_ls = [text_emb_ls[idx] + [text_emb_dense[idx].unsqueeze(0)] for idx in range(len(text_emb_ls))]\n",
    "                \n",
    "            \n",
    "    if args.add_sparse_index:\n",
    "        _, query_sparse_index = construct_dense_or_sparse_encodings_queries(queries, text_model, args.add_sparse_index)\n",
    "        store_sparse_index(samples_hash, query_sparse_index, encoding_query = True)\n",
    "        # text_emb_ls = text_retrieval_model.model.encode_queries(queries, convert_to_tensor=True)\n",
    "\n",
    "        run_search_with_sparse_index(samples_hash)\n",
    "        sparse_sim_scores = read_trec_run(samples_hash, len(queries), len(corpus))\n",
    "    \n",
    "    # retrieve_by_full_query(img_emb, text_emb_ls)\n",
    "    \n",
    "    # if args.is_img_retrieval:\n",
    "    retrieval_model = DRES(batch_size=16, algebra_method=args.algebra_method, is_img_retrieval=(args.is_img_retrieval or not args.dataset_name == \"webis-touche2020\"), prob_agg=args.prob_agg, dependency_topk=args.dependency_topk)\n",
    "    # retrieval_model = DRES(batch_size=16, algebra_method=args.algebra_method, is_img_retrieval=True, prob_agg=args.prob_agg, dependency_topk=args.dependency_topk)\n",
    "    # else:\n",
    "    #     retrieval_model = DRES(models.SentenceBERT(\"msmarco-distilbert-base-tas-b\"), batch_size=16, algebra_method=one)\n",
    "    retriever = EvaluateRetrieval(retrieval_model, score_function='cos_sim') # or \"cos_sim\" for cosine similarity\n",
    "    \n",
    "    if args.query_concept:\n",
    "        if args.is_img_retrieval:\n",
    "            text_emb_ls = [[torch.cat(item) for item in items] for items in text_emb_ls]\n",
    "    \n",
    "    # if args.query_concept:\n",
    "    perc_method = \"two\"\n",
    "    if args.retrieval_method == \"ours\":\n",
    "        if not args.img_concept:\n",
    "            if not args.search_by_cluster:\n",
    "                if args.query_concept:\n",
    "                    patch_emb_by_img_ls = [img_emb[idx].view(1,-1) for idx in range(len(img_emb))]\n",
    "                    results=retrieve_by_embeddings(perc_method, full_sub_queries_ls, queries, retriever, patch_emb_by_img_ls, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=None, grouped_sub_q_ids_ls=None, clustering_topk=args.clustering_topk, sparse_sim_scores=sparse_sim_scores, dataset_name=args.dataset_name)\n",
    "                else:\n",
    "                    results=retrieve_by_embeddings(perc_method, full_sub_queries_ls, queries, retriever, img_emb, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=None, grouped_sub_q_ids_ls=None, clustering_topk=args.clustering_topk, sparse_sim_scores=sparse_sim_scores, dataset_name=args.dataset_name)\n",
    "            else:\n",
    "                # results=retrieve_by_embeddings(retriever, img_emb, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, use_clustering=args.search_by_cluster, clustering_info=(cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_unique_sample_ids_ls, cluster_sample_ids_ls, cluster_sub_X_cat_patch_ids_ls, clustering_nbs_mappings), bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=bboxes_overlap_ls, grouped_sub_q_ids_ls=grouped_sub_q_ids_ls, clustering_topk=args.clustering_topk, sparse_sim_scores=sparse_sim_scores)\n",
    "                results=retrieve_by_embeddings(perc_method, full_sub_queries_ls, queries, retriever, patch_emb_by_img_ls, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, use_clustering=args.search_by_cluster, bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=bboxes_overlap_ls, grouped_sub_q_ids_ls=grouped_sub_q_ids_ls,doc_retrieval=retrieval_method, dataset_name=args.dataset_name)\n",
    "        else:\n",
    "            # if args.dataset_name == \"webis-touche2020\":\n",
    "            args.is_img_retrieval = True\n",
    "                \n",
    "            if not args.search_by_cluster:\n",
    "                results=retrieve_by_embeddings(perc_method, full_sub_queries_ls, queries, retriever, patch_emb_by_img_ls, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=bboxes_overlap_ls, grouped_sub_q_ids_ls=grouped_sub_q_ids_ls, clustering_topk=args.clustering_topk, sparse_sim_scores=sparse_sim_scores, dataset_name=args.dataset_name, is_img_retrieval=args.is_img_retrieval)\n",
    "            else:\n",
    "                # results=retrieve_by_embeddings(retriever, patch_emb_by_img_ls, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, use_clustering=args.search_by_cluster, clustering_info=(cluster_sub_X_tensor_ls, cluster_centroid_tensor, cluster_sample_count_ls, cluster_unique_sample_ids_ls, cluster_sample_ids_ls, cluster_sub_X_cat_patch_ids_ls, clustering_nbs_mappings), bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=bboxes_overlap_ls, grouped_sub_q_ids_ls=grouped_sub_q_ids_ls, clustering_topk=args.clustering_topk, sparse_sim_scores=sparse_sim_scores)\n",
    "                # grouped_sub_q_ids_ls, bboxes_overlap_ls, dependency_topk, device, prob_agg, is_img_retrieval\n",
    "                results=retrieve_by_embeddings(perc_method, full_sub_queries_ls, queries, retriever, patch_emb_by_img_ls, text_emb_ls, qrels, query_count=args.query_count, parallel=args.parallel, use_clustering=args.search_by_cluster, bboxes_ls=bboxes_ls, img_file_name_ls=img_file_name_ls, bboxes_overlap_ls=bboxes_overlap_ls, clustering_topk=args.clustering_topk, grouped_sub_q_ids_ls=grouped_sub_q_ids_ls,doc_retrieval=retrieval_method, prob_agg=args.prob_agg, dependency_topk=args.dependency_topk, device=device, is_img_retrieval=args.is_img_retrieval, method=args.algebra_method, index_method=args.index_method, _nprobe_query=args.nprobe_query, dataset_name=args.dataset_name)\n",
    "    # elif args.retrieval_method == \"llm_ranker\":\n",
    "    #     ranker = LLM_ranker(corpus)\n",
    "    #     results = ranker.retrieval(queries)\n",
    "    #     ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values, ignore_identical_ids=False)\n",
    "    elif args.retrieval_method == \"bm25\":\n",
    "        ranker = BuildIndex(samples_hash, corpus)\n",
    "        t1 = time.time()\n",
    "        results=ranker.retrieval(queries)\n",
    "        t2 = time.time()\n",
    "        print(\"retrieval time::\", t2 - t1)\n",
    "        ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values, ignore_identical_ids=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid retrieval method\")\n",
    "\n",
    "    img_folder = '/root/autodl-fs/datasets/multiqa/'\n",
    "    img_list = os.listdir(img_folder)\n",
    "    idx = int(list(results['1'].keys())[0])\n",
    "    img = img_list[idx - 1]\n",
    "\n",
    "    image_f = []\n",
    "    system = 'You are a helpful assistant. Please answer the question according to the question and picture. You can only answer one word.'\n",
    "\n",
    "    uri = '/root/autodl-fs/datasets/multiqa/' + img\n",
    "    # display(Image(filename=uri, width=300))\n",
    "    \n",
    "    model_path = \"/root/autodl-tmp/LLaVA/llava-v1.5-7b\"\n",
    "    prompt = system + query\n",
    "    \n",
    "    image_f.append(uri)\n",
    "    return infer_once(prompt, image_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebbe222-e017-4caf-97b0-ea05c0ece41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results with decomposition::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:50:33,463 - Encoding Queries...\n",
      "2024-12-03 16:50:33,464 - Sorting Corpus by document length (Longest first)...\n",
      "2024-12-03 16:50:33,486 - Encoding Corpus in batches... Warning: This might take a while!\n",
      "2024-12-03 16:50:33,487 - Scoring Function: Cosine Similarity (cos_sim)\n",
      "100%|██████████| 57025/57025 [00:21<00:00, 2646.36it/s]\n",
      "2024-12-03 16:50:55,168 - \n",
      "\n",
      "2024-12-03 16:50:55,169 - NDCG@1: 0.0000\n",
      "2024-12-03 16:50:55,171 - NDCG@3: 0.0000\n",
      "2024-12-03 16:50:55,172 - NDCG@5: 0.0000\n",
      "2024-12-03 16:50:55,173 - NDCG@10: 0.0000\n",
      "2024-12-03 16:50:55,174 - NDCG@100: 0.0000\n",
      "2024-12-03 16:50:55,175 - NDCG@1000: 0.0000\n",
      "2024-12-03 16:50:55,176 - \n",
      "\n",
      "2024-12-03 16:50:55,177 - MAP@1: 0.0000\n",
      "2024-12-03 16:50:55,178 - MAP@3: 0.0000\n",
      "2024-12-03 16:50:55,178 - MAP@5: 0.0000\n",
      "2024-12-03 16:50:55,179 - MAP@10: 0.0000\n",
      "2024-12-03 16:50:55,180 - MAP@100: 0.0000\n",
      "2024-12-03 16:50:55,181 - MAP@1000: 0.0000\n",
      "2024-12-03 16:50:55,182 - \n",
      "\n",
      "2024-12-03 16:50:55,183 - Recall@1: 0.0000\n",
      "2024-12-03 16:50:55,183 - Recall@3: 0.0000\n",
      "2024-12-03 16:50:55,184 - Recall@5: 0.0000\n",
      "2024-12-03 16:50:55,185 - Recall@10: 0.0000\n",
      "2024-12-03 16:50:55,186 - Recall@100: 0.0000\n",
      "2024-12-03 16:50:55,187 - Recall@1000: 0.0000\n",
      "2024-12-03 16:50:55,188 - \n",
      "\n",
      "2024-12-03 16:50:55,189 - P@1: 0.0000\n",
      "2024-12-03 16:50:55,190 - P@3: 0.0000\n",
      "2024-12-03 16:50:55,190 - P@5: 0.0000\n",
      "2024-12-03 16:50:55,191 - P@10: 0.0000\n",
      "2024-12-03 16:50:55,192 - P@100: 0.0000\n",
      "2024-12-03 16:50:55,193 - P@1000: 0.0000\n",
      "2024-12-03 16:50:55,195 - \n",
      "\n",
      "2024-12-03 16:50:55,196 - NDCG@1: 0.0000\n",
      "2024-12-03 16:50:55,197 - NDCG@3: 0.0000\n",
      "2024-12-03 16:50:55,198 - NDCG@5: 0.0000\n",
      "2024-12-03 16:50:55,199 - NDCG@10: 0.0000\n",
      "2024-12-03 16:50:55,200 - NDCG@100: 0.0000\n",
      "2024-12-03 16:50:55,201 - NDCG@1000: 0.0000\n",
      "2024-12-03 16:50:55,202 - \n",
      "\n",
      "2024-12-03 16:50:55,203 - MAP@1: 0.0000\n",
      "2024-12-03 16:50:55,204 - MAP@3: 0.0000\n",
      "2024-12-03 16:50:55,214 - MAP@5: 0.0000\n",
      "2024-12-03 16:50:55,215 - MAP@10: 0.0000\n",
      "2024-12-03 16:50:55,216 - MAP@100: 0.0000\n",
      "2024-12-03 16:50:55,217 - MAP@1000: 0.0000\n",
      "2024-12-03 16:50:55,218 - \n",
      "\n",
      "2024-12-03 16:50:55,219 - Recall@1: 0.0000\n",
      "2024-12-03 16:50:55,220 - Recall@3: 0.0000\n",
      "2024-12-03 16:50:55,221 - Recall@5: 0.0000\n",
      "2024-12-03 16:50:55,222 - Recall@10: 0.0000\n",
      "2024-12-03 16:50:55,222 - Recall@100: 0.0000\n",
      "2024-12-03 16:50:55,223 - Recall@1000: 0.0000\n",
      "2024-12-03 16:50:55,224 - \n",
      "\n",
      "2024-12-03 16:50:55,225 - P@1: 0.0000\n",
      "2024-12-03 16:50:55,226 - P@3: 0.0000\n",
      "2024-12-03 16:50:55,227 - P@5: 0.0000\n",
      "2024-12-03 16:50:55,228 - P@10: 0.0000\n",
      "2024-12-03 16:50:55,229 - P@100: 0.0000\n",
      "2024-12-03 16:50:55,234 - P@1000: 0.0000\n",
      "2024-12-03 16:50:55,237 - \n",
      "\n",
      "2024-12-03 16:50:55,238 - NDCG@1: 0.0000\n",
      "2024-12-03 16:50:55,239 - NDCG@3: 0.0000\n",
      "2024-12-03 16:50:55,240 - NDCG@5: 0.0000\n",
      "2024-12-03 16:50:55,240 - NDCG@10: 0.0000\n",
      "2024-12-03 16:50:55,241 - NDCG@100: 0.0000\n",
      "2024-12-03 16:50:55,242 - NDCG@1000: 0.0000\n",
      "2024-12-03 16:50:55,243 - \n",
      "\n",
      "2024-12-03 16:50:55,244 - MAP@1: 0.0000\n",
      "2024-12-03 16:50:55,244 - MAP@3: 0.0000\n",
      "2024-12-03 16:50:55,245 - MAP@5: 0.0000\n",
      "2024-12-03 16:50:55,246 - MAP@10: 0.0000\n",
      "2024-12-03 16:50:55,247 - MAP@100: 0.0000\n",
      "2024-12-03 16:50:55,248 - MAP@1000: 0.0000\n",
      "2024-12-03 16:50:55,249 - \n",
      "\n",
      "2024-12-03 16:50:55,250 - Recall@1: 0.0000\n",
      "2024-12-03 16:50:55,250 - Recall@3: 0.0000\n",
      "2024-12-03 16:50:55,252 - Recall@5: 0.0000\n",
      "2024-12-03 16:50:55,252 - Recall@10: 0.0000\n",
      "2024-12-03 16:50:55,253 - Recall@100: 0.0000\n",
      "2024-12-03 16:50:55,254 - Recall@1000: 0.0000\n",
      "2024-12-03 16:50:55,255 - \n",
      "\n",
      "2024-12-03 16:50:55,256 - P@1: 0.0000\n",
      "2024-12-03 16:50:55,256 - P@3: 0.0000\n",
      "2024-12-03 16:50:55,257 - P@5: 0.0000\n",
      "2024-12-03 16:50:55,258 - P@10: 0.0000\n",
      "2024-12-03 16:50:55,259 - P@100: 0.0000\n",
      "2024-12-03 16:50:55,260 - P@1000: 0.0000\n",
      "2024-12-03 16:50:55,262 - \n",
      "\n",
      "2024-12-03 16:50:55,263 - NDCG@1: 0.0000\n",
      "2024-12-03 16:50:55,264 - NDCG@3: 0.0000\n",
      "2024-12-03 16:50:55,265 - NDCG@5: 0.0000\n",
      "2024-12-03 16:50:55,266 - NDCG@10: 0.0000\n",
      "2024-12-03 16:50:55,267 - NDCG@100: 0.0000\n",
      "2024-12-03 16:50:55,267 - NDCG@1000: 0.0000\n",
      "2024-12-03 16:50:55,268 - \n",
      "\n",
      "2024-12-03 16:50:55,269 - MAP@1: 0.0000\n",
      "2024-12-03 16:50:55,270 - MAP@3: 0.0000\n",
      "2024-12-03 16:50:55,270 - MAP@5: 0.0000\n",
      "2024-12-03 16:50:55,271 - MAP@10: 0.0000\n",
      "2024-12-03 16:50:55,272 - MAP@100: 0.0000\n",
      "2024-12-03 16:50:55,273 - MAP@1000: 0.0000\n",
      "2024-12-03 16:50:55,274 - \n",
      "\n",
      "2024-12-03 16:50:55,274 - Recall@1: 0.0000\n",
      "2024-12-03 16:50:55,275 - Recall@3: 0.0000\n",
      "2024-12-03 16:50:55,276 - Recall@5: 0.0000\n",
      "2024-12-03 16:50:55,277 - Recall@10: 0.0000\n",
      "2024-12-03 16:50:55,277 - Recall@100: 0.0000\n",
      "2024-12-03 16:50:55,278 - Recall@1000: 0.0000\n",
      "2024-12-03 16:50:55,279 - \n",
      "\n",
      "2024-12-03 16:50:55,279 - P@1: 0.0000\n",
      "2024-12-03 16:50:55,280 - P@3: 0.0000\n",
      "2024-12-03 16:50:55,281 - P@5: 0.0000\n",
      "2024-12-03 16:50:55,282 - P@10: 0.0000\n",
      "2024-12-03 16:50:55,282 - P@100: 0.0000\n",
      "2024-12-03 16:50:55,283 - P@1000: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1758e-05, 1.5766e-05, 1.5894e-05,  ..., 1.2172e-05, 3.5090e-05,\n",
      "         1.0242e-05]])\n",
      "Time taken: 21.70s\n",
      "These are the queries:\n",
      "Option 2: length of sentence\n",
      "This is query_lengths:\n",
      "[40]\n",
      "These are the percentiles:\n",
      "[9.9]\n",
      "defaultdict(<class 'list'>, {9: [0], 9.5: [0], 9.9: [0]})\n",
      "Percentile range 90-100:\n",
      "1 queries in this percentile range\n",
      "Percentile range 95-100:\n",
      "1 queries in this percentile range\n",
      "Percentile range 99-100:\n",
      "1 queries in this percentile range\n",
      "Overall recall by buckets:\n",
      "90-100: {'Recall@1': 0.0, 'Recall@3': 0.0, 'Recall@5': 0.0, 'Recall@10': 0.0, 'Recall@100': 0.0, 'Recall@1000': 0.0}\n",
      "95-100: {'Recall@1': 0.0, 'Recall@3': 0.0, 'Recall@5': 0.0, 'Recall@10': 0.0, 'Recall@100': 0.0, 'Recall@1000': 0.0}\n",
      "99-100: {'Recall@1': 0.0, 'Recall@3': 0.0, 'Recall@5': 0.0, 'Recall@10': 0.0, 'Recall@100': 0.0, 'Recall@1000': 0.0}\n",
      "Overall scores:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Blue', '117d500aaa630023c4038b8268b309c0.jpg')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'What color is the Santa Anita Park logo?'\n",
    "\n",
    "query_once(q, patch_emb_by_img_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc53494a-97b2-4694-a479-41412d2e3e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='chat-d2270c92df9c43b690ed7528b3a544f2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and Schrödinger\\'s cat?\" \\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1735050000, model='/root/autodl-tmp/Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=55, prompt_tokens=46, total_tokens=101, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8010/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"/root/autodl-tmp/Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97b6102-9bd8-4b4b-9632-a2dad42a1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "vlm = ChatOpenAI(\n",
    "    model=\"/root/autodl-tmp/llava-1.5-7b-hf\",\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= \"http://localhost:8000/v1\",\n",
    "    max_tokens=25,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57fa48bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Message dict must contain 'role' and 'content' keys, got {'prompt': 'You are a helpful assistant. Please answer the question according to the question and picture. You can only answer one word.What sport is being played in one of the photos in this collage of Pittsburgh?', 'multi_modal_data': {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=759x815 at 0x7F1E999A52D0>}}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/messages/utils.py:319\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     msg_type \u001b[38;5;241m=\u001b[39m \u001b[43mmsg_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'role'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/messages/utils.py:321\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     msg_type \u001b[38;5;241m=\u001b[39m \u001b[43mmsg_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# None msg content is not allowed\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# outputs = vlm.generate({\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     \"prompt\": prompt,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     \"multi_modal_data\": {\"image\": image},\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulti_modal_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:287\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 287\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:267\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StringPromptValue(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, Sequence):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39m\u001b[43mconvert_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/messages/utils.py:357\u001b[0m, in \u001b[0;36mconvert_to_messages\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_convert_to_message(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/messages/utils.py:357\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain_core/messages/utils.py:329\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    325\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage dict must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         msg \u001b[38;5;241m=\u001b[39m create_message(\n\u001b[1;32m    327\u001b[0m             message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mMESSAGE_COERCION_FAILURE\n\u001b[1;32m    328\u001b[0m         )\n\u001b[0;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     _message \u001b[38;5;241m=\u001b[39m _create_message_from_message_type(\n\u001b[1;32m    331\u001b[0m         msg_type, msg_content, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmsg_kwargs\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Message dict must contain 'role' and 'content' keys, got {'prompt': 'You are a helpful assistant. Please answer the question according to the question and picture. You can only answer one word.What sport is being played in one of the photos in this collage of Pittsburgh?', 'multi_modal_data': {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=759x815 at 0x7F1E999A52D0>}}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "system = 'You are a helpful assistant. Please answer the question according to the question and picture. You can only answer one word.'\n",
    "prompt = system + 'What sport is being played in one of the photos in this collage of Pittsburgh?'\n",
    "\n",
    "message = f\"USER: <image>\\n{prompt}\\nASSISTANT:\"\n",
    "\n",
    "img = '/root/autodl-fs/datasets/multiqa/' + '7bbaf1d3e20828038f2d3d21f000fea7.JPG'\n",
    "\n",
    "image = Image.open(img)\n",
    "# outputs = vlm.generate({\n",
    "#     \"prompt\": prompt,\n",
    "#     \"multi_modal_data\": {\"image\": image},\n",
    "# })\n",
    "outputs = vlm.invoke([{\n",
    "     \"prompt\": prompt,\n",
    "     \"multi_modal_data\": {\"image\": image},\n",
    " }])\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfda3f3-addd-4211-81df-446004428450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for o in outputs:\n",
    "    generated_text = o.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbed42-62a2-44ae-8c4a-0159ba586b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46fb08d-a5a9-4732-a49b-282977337a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T15:57:13.685122Z",
     "iopub.status.busy": "2025-01-09T15:57:13.683829Z",
     "iopub.status.idle": "2025-01-09T15:57:15.480632Z",
     "shell.execute_reply": "2025-01-09T15:57:15.479613Z",
     "shell.execute_reply.started": "2025-01-09T15:57:13.685034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icml01/anaconda3/envs/rag/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb7dfe7-d38b-4eef-8a30-ccb2a4227078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T16:01:24.055172Z",
     "iopub.status.busy": "2025-01-09T16:01:24.053977Z",
     "iopub.status.idle": "2025-01-09T16:01:24.064988Z",
     "shell.execute_reply": "2025-01-09T16:01:24.063498Z",
     "shell.execute_reply.started": "2025-01-09T16:01:24.055082Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7405"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a701dae3-3590-472d-9bd5-16514e043514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T16:06:40.693479Z",
     "iopub.status.busy": "2025-01-09T16:06:40.691896Z",
     "iopub.status.idle": "2025-01-09T16:06:40.722037Z",
     "shell.execute_reply": "2025-01-09T16:06:40.720884Z",
     "shell.execute_reply.started": "2025-01-09T16:06:40.693388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = sum(1 for row in loaded_data if len(row) > 2)\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ab1a4-09a4-48f6-91e6-4e56f38d1c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
