{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image_path = \"crepe_data/VG_100K_2/1.jpg\"\n",
    "# image = mpimg.imread(image_path)\n",
    "image = Image.open(image_path)\n",
    "image = image.convert('RGB')\n",
    "\n",
    "# plt.title(\"Sheep Image\")\n",
    "# plt.xlabel(\"X pixel scaling\")\n",
    "# plt.ylabel(\"Y pixel scaling\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "target_image_id=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_caption_file_name=\"crepe_data/prod_hard_negatives/prod_vg_hard_negs_swap_all.csv\"\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "caption_pd = pd.read_csv(img_caption_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(caption_pd.iloc[4]['caption'], caption_pd.iloc[4]['image_id'])\n",
    "print(caption_pd.iloc[3]['caption'], caption_pd.iloc[3]['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(caption_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import load_crepe_datasets, load_other_crepe_images\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "\n",
    "data_path=\"/data2/wuyinjun/\"\n",
    "query_path=os.getcwd() #curr_dir = str(globals()[\"_dh\"][0])\n",
    "dataset_name=\"crepe\"\n",
    "full_data_path = os.path.join(data_path, dataset_name)\n",
    "# raw_img_ls = [image]\n",
    "queries, raw_img_ls, sub_queries_ls, img_idx_ls = load_crepe_datasets(full_data_path, query_path)\n",
    "img_idx_ls, raw_img_ls = load_other_crepe_images(full_data_path, query_path, img_idx_ls, raw_img_ls, total_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(raw_img_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = ViTImageProcessor.from_pretrained('google/vit-large-patch16-224')\n",
    "# model = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224').to(device)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "# processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "raw_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\", padding=False, do_resize=False, do_center_crop=False)[\"pixel_values\"]\n",
    "processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "text_processor =  lambda text: raw_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "img_processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import *\n",
    "\n",
    "new_raw_img_ls = raw_img_ls #[raw_img_ls[idx] for idx in range(1)]\n",
    "\n",
    "cl = ConceptLearner(new_raw_img_ls, model, vit_forward, processor, dataset_name, device)\n",
    "\n",
    "patch_count=4\n",
    "\n",
    "img_emb, patch_emb, masks, bboxes, img_per_patch = cl.get_patches(images=new_raw_img_ls, n_patches=patch_count, method=\"slic\", compute_img_emb=True)\n",
    "\n",
    "# img_emb, patch_emb_ls, masks_ls, bboxes_ls, img_per_patch_ls = convert_samples_to_concepts(args, model, raw_img_ls, processor, device, patch_count_ls=patch_count_ls)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL\n",
    "import ipyplot\n",
    "\n",
    "from tqdm import tqdm\n",
    "target_image_id  =15\n",
    "print(\"target_image_id::\", target_image_id)\n",
    "image_id = target_image_id\n",
    "\n",
    "full_masked_image_ls = []\n",
    "\n",
    "for image_id in tqdm(range(len(raw_img_ls))):\n",
    "# for image_id in tqdm(range(1)):\n",
    "\n",
    "    first_mask = masks[image_id]\n",
    "\n",
    "    # print(len(first_mask))\n",
    "\n",
    "    first_img = raw_img_ls[image_id]\n",
    "\n",
    "    first_bbox_ls = bboxes[image_id]\n",
    "\n",
    "    # print(len(first_bbox_ls))\n",
    "\n",
    "    masked_image_ls = []\n",
    "\n",
    "    for idx in range(len(first_bbox_ls)):\n",
    "        first_bbox = first_bbox_ls[idx]\n",
    "        masked_image = np.copy(first_img)\n",
    "        # # print(masked_image.shape)\n",
    "        masked_image[first_mask != (idx + 1)] = 255 # Set pixels outside the mask to 0\n",
    "        # curr_patch = PIL.Image.new('RGB', first_img.size)\n",
    "        # curr_patch.paste(first_img.copy().crop(first_bbox), box=first_bbox)\n",
    "        # # masked_image = PIL.ImageOps.pad(curr_patch.crop(first_bbox), (224, 224))\n",
    "        # masked_image = curr_patch.crop(first_bbox)\n",
    "        masked_image_ls.append(masked_image)\n",
    "\n",
    "    full_masked_image_ls.append(masked_image_ls)\n",
    "\n",
    "ipyplot.plot_images(full_masked_image_ls[target_image_id], max_images=len(bboxes[target_image_id]))   \n",
    "\n",
    "# masked_image = cv2.rectangle(Image.fromarray(masked_image), (first_bbox[0], first_bbox[1]), (first_bbox[2], first_bbox[3]), (255, 0, 0), 2)\n",
    "\n",
    "# Display the masked part of the image\n",
    "# plt.imshow(masked_image)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# print(np.unique(first_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mask = masks[0]\n",
    "first_img = raw_img_ls[0]\n",
    "first_bbox_ls = bboxes[0]\n",
    "print(np.unique(first_mask))\n",
    "\n",
    "first_img_cp = first_img.copy()\n",
    "first_img_cp_array = np.array(first_img_cp)\n",
    "first_img_cp_array[first_mask!=2] = 0\n",
    "\n",
    "plt.imshow(Image.fromarray(first_img_cp_array))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "further_masks = get_slic_segments([Image.fromarray(first_img_cp_array)], n_segments=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(further_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img_cp_array_cp1 = np.copy(first_img_cp_array)\n",
    "first_img_cp_array_cp1[further_masks[0] != 5] = 100\n",
    "plt.imshow(Image.fromarray(first_img_cp_array_cp1))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_idx_ls = np.nonzero(np.array(img_per_patch) == target_image_id)[0].tolist()\n",
    "first_patch_emb = np.stack([patch_emb[idx] for idx in sub_idx_ls])\n",
    "print(first_patch_emb.shape)\n",
    "print(len(bboxes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"the street has a parking meter and a lamp post\"\n",
    "# query=\"sign on a building with a window and balcony\"\n",
    "# query=\"sign on a building with a window and balcony\"#. \n",
    "query=\"the door has a frame around it, and there is a second window on the building.\"\n",
    "inputs = text_processor(query)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "text_features = model.get_text_features(**inputs)\n",
    "print(text_features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segment images with slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patch_cos_sim_ls = []\n",
    "all_img_cos_sim_ls = []\n",
    "\n",
    "for image_id in tqdm(range(len(raw_img_ls))):\n",
    "\n",
    "    sub_idx_ls = np.nonzero(np.array(img_per_patch) == image_id)[0].tolist()\n",
    "    first_patch_emb = np.stack([patch_emb[idx] for idx in sub_idx_ls])\n",
    "\n",
    "    # first_bbox_ls = bboxes[image_id]\n",
    "    patch_cos_sim_ls = []\n",
    "    for idx in range(len(first_patch_emb)):\n",
    "        cosin_sim1 = torch.nn.functional.cosine_similarity(text_features, torch.from_numpy(first_patch_emb[idx]).view(1,-1).to(device), dim=-1)\n",
    "        patch_cos_sim_ls.append(cosin_sim1)\n",
    "        # print(idx, cosin_sim1)\n",
    "    cosin_sim2 = torch.nn.functional.cosine_similarity(text_features, img_emb[image_id].view(1,-1).to(device), dim=-1)\n",
    "    all_img_cos_sim_ls.append(cosin_sim2)\n",
    "    all_patch_cos_sim_ls.append(torch.tensor(patch_cos_sim_ls))\n",
    "\n",
    "# print(cosin_sim1, cosin_sim2)\n",
    "all_img_cos_sim_tensor = torch.tensor(all_img_cos_sim_ls)\n",
    "# all_patch_cos_sim_tensor = torch.tensor(all_patch_cos_sim_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_img_cos_sim_tensor.argmax())\n",
    "print(all_img_cos_sim_tensor.max())\n",
    "all_patch_cos_sim_max_scores = torch.tensor([torch.max(all_patch_cos_sim_ls[idx]) for idx in range(len(all_patch_cos_sim_ls))])\n",
    "print(torch.argmax(all_patch_cos_sim_max_scores))\n",
    "print(torch.max(all_patch_cos_sim_max_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img_sorted_ids = torch.sort(all_img_cos_sim_tensor, descending=True)[1]\n",
    "print(torch.nonzero(full_img_sorted_ids == target_image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patch_cos_sim_max_scores_full = torch.stack([all_patch_cos_sim_max_scores.view(-1), all_img_cos_sim_tensor.view(-1)], dim=1)\n",
    "all_patch_cos_sim_max_scores_full_final = torch.max(all_patch_cos_sim_max_scores_full, dim=1)[0]\n",
    "print(all_patch_cos_sim_max_scores_full_final)\n",
    "sorted_ids = torch.sort(all_patch_cos_sim_max_scores_full_final, descending=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_ids)\n",
    "print(all_patch_cos_sim_max_scores_full_final[target_image_id])\n",
    "print(torch.nonzero(sorted_ids == target_image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(all_patch_cos_sim_ls[target_image_id]), torch.argmax(all_patch_cos_sim_ls[target_image_id]))\n",
    "print(torch.sort(all_patch_cos_sim_ls[target_image_id], descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = raw_img_ls[target_image_id].convert('RGB')\n",
    "\n",
    "# plt.title(\"Sheep Image\")\n",
    "# plt.xlabel(\"X pixel scaling\")\n",
    "# plt.ylabel(\"Y pixel scaling\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(first_mask))\n",
    "print(np.sum(first_mask == 15), np.sum(first_mask == 4))\n",
    "compound_mask = np.logical_or(first_mask == 15, first_mask == 4)\n",
    "\n",
    "print(np.sum(compound_mask))\n",
    "\n",
    "y_ls, x_ls = np.nonzero(compound_mask)\n",
    "\n",
    "x1, y1, x2, y2 = np.min(x_ls), np.min(y_ls), np.max(x_ls), np.max(y_ls)\n",
    "compound_box = [x1, y1, x2, y2]\n",
    "print(compound_box)\n",
    "\n",
    "\n",
    "curr_patch = PIL.Image.new('RGB', first_img.size)\n",
    "curr_patch.paste(first_img.copy().crop(compound_box), box=compound_box)\n",
    "masked_image = PIL.ImageOps.pad(curr_patch.crop(compound_box), (224, 224))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.imshow(masked_image)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "masked_image_emb, masked_img_for_patch = get_patches_from_bboxes(cl.input_to_latent, cl.model, [first_img], [[compound_box]], cl.input_processor, device=cl.device, resize=cl.image_size)\n",
    "print(masked_image_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_sim3 = torch.nn.functional.cosine_similarity(text_features, masked_image_emb.view(1,-1).to(device), dim=-1)\n",
    "print(cosin_sim3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segment images with sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "\n",
    "def segment_one_image(image):\n",
    "    checkpoint_path=\"/data2/wuyinjun/sam/sam_vit_h_4b8939.pth\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sam = sam_model_registry[\"vit_h\"](checkpoint=checkpoint_path)\n",
    "    sam.to(device=device)\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam\n",
    "    , points_per_side=16)\n",
    "    masks = mask_generator.generate(image)\n",
    "\n",
    "    # print(len(masks))\n",
    "    return masks\n",
    "\n",
    "\n",
    "def segment_image_ls(image_ls):\n",
    "    checkpoint_path=\"/data2/wuyinjun/sam/sam_vit_h_4b8939.pth\"\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sam = sam_model_registry[\"vit_h\"](checkpoint=checkpoint_path)\n",
    "    sam.to(device=device)\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam , points_per_side=16)\n",
    "    masks_ls = []\n",
    "    for image in tqdm(image_ls):\n",
    "        masks = mask_generator.generate(np.array(image))\n",
    "        masks_ls.append(masks)\n",
    "\n",
    "    # print(len(masks))\n",
    "    return masks_ls\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# masks_ls = []\n",
    "# for idx in tqdm(range(len(raw_img_ls))):\n",
    "#     masks = segment_one_image(np.array(raw_img_ls[idx]))\n",
    "#     masks_ls.append(masks)\n",
    "masks_ls = segment_image_ls(raw_img_ls)\n",
    "# predictor = SamPredictor(sam)\n",
    "# predictor.set_image(raw_img_ls[0])\n",
    "# masks, _, _ = predictor.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(masks_ls, \"output/crepe_masks.pkl\")\n",
    "# utils.dump_pickle(\"crepe_masks.pkl\", masks_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_ls = utils.load(\"output/crepe_masks.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "full_masked_image_emb_ls = []\n",
    "print(\"total image count::\", len(raw_img_ls))\n",
    "for idx in range(len(raw_img_ls)):\n",
    "    print(\"image idx::\", idx)\n",
    "    masks = masks_ls[idx]\n",
    "    masked_image_ls = []\n",
    "    for mask in masks:\n",
    "        # print(mask.shape)\n",
    "        # print(np.unique(mask['segmentation']))\n",
    "        y_ls, x_ls = np.nonzero(mask['segmentation'])\n",
    "        bbox = [np.min(x_ls), np.min(y_ls), np.max(x_ls), np.max(y_ls)]\n",
    "        curr_patch = PIL.Image.new('RGB', raw_img_ls[idx].size)\n",
    "        curr_patch.paste(raw_img_ls[idx].copy().crop(bbox), box=bbox)\n",
    "        # masked_image0 = curr_patch # PIL.ImageOps.pad(curr_patch.crop(bbox), (224, 224))\n",
    "        # masked_image = PIL.ImageOps.pad(curr_patch.crop(bbox), curr_patch.size) \n",
    "        masked_image = curr_patch.crop(bbox).convert('RGB')\n",
    "        if masked_image.size[0] > 1 and masked_image.size[1] > 1:\n",
    "            masked_image_ls.append(masked_image)\n",
    "        else:\n",
    "            if masked_image.size[0] == 1:\n",
    "                masked_image_ls.append(masked_image.resize((2, masked_image.size[1])))\n",
    "            else:\n",
    "                masked_image_ls.append(masked_image.resize((masked_image.size[0], 2)))\n",
    "    \n",
    "    # print(masked_image_ls[63].size)\n",
    "    # print(masked_image_ls[62].size)\n",
    "    masked_image_emb_tensor = get_image_embeddings(masked_image_ls, cl.input_processor, cl.input_to_latent, cl.model, not_normalize=False)\n",
    "    \n",
    "    # masked_image_emb_ls.append(masked_image_emb.view(-1))\n",
    "        \n",
    "    # masked_image_emb_tensor = torch.stack(masked_image_emb_ls)\n",
    "    \n",
    "    full_masked_image_emb_ls.append(masked_image_emb_tensor)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save(full_masked_image_emb_ls, \"output/crepe_masked_image_emb_ls.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_masked_image_emb_ls = utils.load(\"output/crepe_masked_image_emb_ls.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patch_cos_sim_ls2 = []\n",
    "# all_img_cos_sim_ls = []\n",
    "\n",
    "for image_id in tqdm(range(len(raw_img_ls))):\n",
    "\n",
    "    sub_idx_ls = np.nonzero(np.array(img_per_patch) == image_id)[0].tolist()\n",
    "    # first_patch_emb = np.stack([patch_emb[idx] for idx in sub_idx_ls])\n",
    "    masked_image_emb_ls = full_masked_image_emb_ls[image_id]\n",
    "\n",
    "    # first_bbox_ls = bboxes[image_id]\n",
    "    patch_cos_sim_ls = []\n",
    "    for idx in range(len(masked_image_emb_ls)):\n",
    "        cosin_sim1 = torch.nn.functional.cosine_similarity(text_features, masked_image_emb_ls[idx].view(1,-1).to(device), dim=-1)\n",
    "        patch_cos_sim_ls.append(cosin_sim1)\n",
    "        # print(idx, cosin_sim1)\n",
    "    # cosin_sim2 = torch.nn.functional.cosine_similarity(text_features, img_emb[image_id].view(1,-1).to(device), dim=-1)\n",
    "    # all_img_cos_sim_ls.append(cosin_sim2)\n",
    "    all_patch_cos_sim_ls2.append(torch.tensor(patch_cos_sim_ls))\n",
    "\n",
    "# print(cosin_sim1, cosin_sim2)\n",
    "# all_img_cos_sim_tensor = torch.tensor(all_img_cos_sim_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patch_cos_sim_max_scores2 = torch.tensor([torch.max(all_patch_cos_sim_ls2[idx]) for idx in range(len(all_patch_cos_sim_ls2))])\n",
    "print(all_patch_cos_sim_max_scores2[15])\n",
    "print(torch.argmax(all_patch_cos_sim_max_scores2))\n",
    "print(torch.max(all_patch_cos_sim_max_scores2))\n",
    "\n",
    "full_img_sorted_ids = torch.sort(all_patch_cos_sim_max_scores2, descending=True)[1]\n",
    "print(torch.nonzero(full_img_sorted_ids == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_sorted_ids = torch.sort(all_img_cos_sim_tensor, descending=True)[1]\n",
    "print(torch.nonzero(all_img_sorted_ids == 0))\n",
    "print(all_img_cos_sim_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(all_patch_cos_sim_ls2[0]), torch.argmax(all_patch_cos_sim_ls2[0]))\n",
    "print(torch.sort(all_patch_cos_sim_ls2[0], descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(masks[0].keys()))\n",
    "print(np.sum(masks[0]['segmentation']), masks[0][\"area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def generate_dialated_mask(mask, dilation_size=5):\n",
    "    kernel = np.ones((dilation_size, dilation_size), np.uint8)\n",
    "\n",
    "    binary_image = np.array(mask).astype(np.uint8)\n",
    "\n",
    "    # Perform dilation\n",
    "    dilated_image = cv2.dilate(binary_image, kernel, iterations=1)\n",
    "    # from skimage.morphology import disk, dilation\n",
    "    # selem = disk(dilation_size)\n",
    "    # return dilation(mask, selem)\n",
    "    return dilated_image\n",
    "\n",
    "\n",
    "def generate_dialated_mask_ls(segmentations, dilation_size=10):\n",
    "    return [generate_dialated_mask(segment['segmentation'], dilation_size) for segment in segmentations]\n",
    "\n",
    "\n",
    "dilated_masks = generate_dialated_mask_ls(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "    \n",
    "import cv2\n",
    "image = np.array(raw_img_ls[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "masked_image = np.copy(raw_img_ls[0])\n",
    "# masked_image[np.logical_and(~masks[2]['segmentation'], ~masks[1]['segmentation'])] = 255\n",
    "# masked_image[~masks[4]['segmentation']] = 255\n",
    "plt.imshow(masked_image)\n",
    "# show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def are_segments_neighbors(seg1_mask, seg2_mask):\n",
    "    \"\"\"Check if two image segments are neighbors using connected component labeling.\"\"\"\n",
    "    # Find contours for the segments\n",
    "    # seg1_mask = Image.fromarray(seg1_mask)\n",
    "    # seg2_mask = Image.fromarray(seg2_mask)\n",
    "    \n",
    "    contours1, _ = cv2.findContours(seg1_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours2, _ = cv2.findContours(seg2_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if any contour from one segment intersects with any contour from the other segment\n",
    "    for contour1 in contours1:\n",
    "        for contour2 in contours2:\n",
    "            intersection = cv2.pointPolygonTest(contour2, tuple(contour1[0][0]), measureDist=False)\n",
    "            if intersection == 0 or intersection == 1:\n",
    "                return True  # Contours intersect, segments are neighbors\n",
    "    return False\n",
    "\n",
    "def are_segments_neighbors0(seg1_mask, seg2_mask):\n",
    "    intersection = np.logical_and(seg1_mask, seg2_mask)\n",
    "    return np.any(intersection)\n",
    "\n",
    "\n",
    "# is_neighbor = are_segments_neighbors(masks[1]['segmentation'].astype(np.uint8), masks[2]['segmentation'].astype(np.uint8))\n",
    "is_neighbor = are_segments_neighbors0(dilated_masks[1], dilated_masks[2])\n",
    "print(is_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(np.array(raw_img_ls[0]),cv2.COLOR_BGR2GRAY)\n",
    "_,binary = cv2.threshold(gray,100,255,cv2.THRESH_BINARY)\n",
    "\n",
    "image,contours = cv2.findContours(binary,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary.dtype)\n",
    "print(masks[1]['segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_all_neighbors(mask_ls):\n",
    "\n",
    "    neighbor_mapping_ls = []\n",
    "    for i in range(len(mask_ls)):\n",
    "        curr_neighbors = []\n",
    "        for j in range(len(mask_ls)):\n",
    "            if j != i:\n",
    "                # is_neighbor = are_segments_neighbors(masks[i]['segmentation'], masks[j]['segmentation'])\n",
    "                is_neighbor = are_segments_neighbors0(mask_ls[i], mask_ls[j])\n",
    "                if is_neighbor:\n",
    "                    curr_neighbors.append(j)\n",
    "        neighbor_mapping_ls.append(curr_neighbors)\n",
    "    return neighbor_mapping_ls\n",
    "\n",
    "neighbor_mapping_ls = derive_all_neighbors(dilated_masks)\n",
    "print(neighbor_mapping_ls[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mask = np.zeros(masks[0]['segmentation'].shape).astype(bool)\n",
    "for idx in neighbor_mapping_ls[4]:\n",
    "    print(idx)\n",
    "    all_mask = np.logical_or(all_mask, masks[idx]['segmentation'])\n",
    "\n",
    "all_mask = np.logical_or(all_mask, masks[4]['segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "binary_image = np.array(masks[1]['segmentation']).astype(np.uint8)\n",
    "\n",
    "# Perform dilation\n",
    "dilated_image = cv2.dilate(binary_image, kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(binary_image))\n",
    "print(np.unique(dilated_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img_ls[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree:0, 10\n",
    "# car:13\n",
    "import ipyplot\n",
    "img_id=0\n",
    "# fig, axes = plt.subplots(10, 1, figsize=(12, 8))\n",
    "\n",
    "def show_all_masked_image_ls(raw_img, masks):\n",
    "\n",
    "        masked_image_ls = []\n",
    "\n",
    "        # for i, ax in enumerate(axes.flat):\n",
    "        for i in range(len(masks)):\n",
    "                # if i < 10:\n",
    "                masked_image = np.copy(raw_img)\n",
    "                masked_image[~masks[i]['segmentation']] = 255\n",
    "                masked_image_ls.append(masked_image)\n",
    "                # masked_image = Image.fromarray(masked_image)\n",
    "                # ax.imshow(masked_image.resize(12, 8))\n",
    "                # ax.axis(\"off\")\n",
    "\n",
    "        ipyplot.plot_images(masked_image_ls, max_images=len(masks))   \n",
    "\n",
    "show_all_masked_image_ls(raw_img_ls[img_id], masks_ls[img_id])\n",
    "        # ax.set_title(f\"Segment {i}\")\n",
    "# plt.show()\n",
    "        # else:\n",
    "        #     ax.axis(\"off\")\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# for idx in range(len(masks)):\n",
    "#     masked_image = np.copy(raw_img_ls[0])\n",
    "#     masked_image[~masks[idx]['segmentation']] = 255\n",
    "# # masked_image[~all_mask.astype(bool)] = 255\n",
    "#     plt.imshow(masked_image)\n",
    "#     plt.title(f\"Segment {idx}\")\n",
    "# # show_anns(masks)\n",
    "#     plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = masks_ls[img_id]\n",
    "all_mask = np.zeros(masks[0]['segmentation'].shape).astype(bool)\n",
    "# mask_id_ls = [2,29,130]\n",
    "# mask_id_ls = range(8,9)\n",
    "# mask_id_ls=[8, 12, 20, 170]#, 173, 149, 134, 2, 3, 44, 5, 6, 9, 8, 15] + list(range(1,100))\n",
    "# mask_id_ls=[20, 12]\n",
    "# mask_id_ls = [3,  12,   0,   6]\n",
    "mask_id_ls = [3,   12]\n",
    "# mask_id_ls=[65,  17,  37, 112]\n",
    "# mask_id_ls=[65]\n",
    "# mask_id_ls=[1]\n",
    "\n",
    "# mask_id_ls = [0,8]\n",
    "# for idx in range(len(masks)):\n",
    "for idx in mask_id_ls:\n",
    "    # print(idx)\n",
    "    all_mask = np.logical_or(all_mask, masks[idx]['segmentation'])\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "# # for idx in range(len(masks)):\n",
    "\n",
    "\n",
    "# masked_image = np.copy(raw_img_ls[img_id])\n",
    "# masked_image[~all_mask] = 255\n",
    "# masked_image = Image.fromarray(masked_image)\n",
    "\n",
    "# y_ls, x_ls = np.nonzero(all_mask)\n",
    "# bbox = [np.min(x_ls), np.min(y_ls), np.max(x_ls), np.max(y_ls)]\n",
    "\n",
    "# curr_patch = PIL.Image.new('RGB', raw_img_ls[img_id].size)\n",
    "# curr_patch.paste(masked_image.copy().crop(bbox), box=bbox)\n",
    "# masked_image0 = curr_patch # PIL.ImageOps.pad(curr_patch.crop(bbox), (224, 224))\n",
    "# masked_image = PIL.ImageOps.pad(curr_patch.crop(bbox), curr_patch.size)\n",
    "\n",
    "\n",
    "\n",
    "y_ls, x_ls = np.nonzero(all_mask)\n",
    "bbox = [np.min(x_ls), np.min(y_ls), np.max(x_ls), np.max(y_ls)]\n",
    "curr_patch = PIL.Image.new('RGB', raw_img_ls[img_id].size)\n",
    "curr_patch.paste(raw_img_ls[img_id].copy().crop(bbox), box=bbox)\n",
    "masked_image0 = curr_patch # PIL.ImageOps.pad(curr_patch.crop(bbox), (224, 224))\n",
    "# masked_image = PIL.ImageOps.pad(curr_patch.crop(bbox), curr_patch.size)\n",
    "masked_image = curr_patch.crop(bbox)#.resize((224, 224))\n",
    "# masked_image[~all_mask.astype(bool)] = 255\n",
    "plt.imshow(masked_image)\n",
    "plt.title(f\"Segment {idx}\")\n",
    "# show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image_emb = get_image_embeddings([masked_image], cl.input_processor, cl.input_to_latent, cl.model, not_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_sim4 = torch.nn.functional.cosine_similarity(text_features, masked_image_emb.view(1,-1).to(device), dim=-1)\n",
    "print(cosin_sim4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_sim4 = torch.nn.functional.cosine_similarity(text_features, img_emb[15].view(1,-1).to(device), dim=-1)\n",
    "print(cosin_sim4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
